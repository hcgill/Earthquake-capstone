---
title: "Earthquake_Capstone_Report"
author: "H. Ewton"
date: "9/28/2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Sources

Two data sets were used in this analysis. The first data set, 
signif_earthquakes, was obtained from NOAA's Significant Earthquakes Database 
(<https://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1>) and lists every 
recorded earthquake in history back to 2150 BC. The other data set used, 
USGS_df, was obtained from Kaggle 
(<https://www.kaggle.com/usgs/earthquake-database#database.csv>) and lists all 
recorded major earthquakes in the USGS data base from 1965 to 2016. 

## signif_earthquakes clean-up
This file, signif_earthquakes, presented several challenges. The file first
needed to be reduced to only include relevant variables from the original 
original data set; these relevant columsn included date information, time, 
magnitude, and location information. Next, the data needed to be filtered to 
include observations that included reliable magnitude measurements. The chosen 
measurement scale for this analysis is the Richter scale as it has been shown to
be reliable and provides substantial amounts of data; therefore, all data prior 
to the creation and regular use of the Richter scale (1935) was removed. 
Earthquakes that were documented without a measurement were also filtered and 
removed from the table. These processes left us with the table below:   


```{r, echo = FALSE, include = FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(sp)
library(geojsonio)
library(maps)
library(ggthemes)
library(gganimate)
library(caTools)

#Import data from NOAA on significant earthquakes in recorded geologic history
signif_earthquakes <- read_delim("signif (3).txt", 
                                 "\t", escape_double = FALSE, trim_ws = TRUE)

#Save as data frame
signif_earthquakes <- as.data.frame(signif_earthquakes, 
                                    stringsAsFactors = FALSE)

#Select relevant headings
signif_earthquakes <- select(signif_earthquakes, "YEAR", "MONTH", "DAY", "HOUR",
                             "MINUTE", "SECOND", "FOCAL_DEPTH", "EQ_MAG_UNK", 
                             "COUNTRY", "LOCATION_NAME", "LATITUDE", 
                             "LONGITUDE")

#Change headings to lowercase
signif_earthquakes <- setNames(signif_earthquakes, 
                               tolower(names(signif_earthquakes)))

#Select data from 1935 to present
signif_earthquakes <- filter(signif_earthquakes, year > 1934)

#Remove any magnitude values that are NA 
#(indicates a recorded earthquake with no measurement)
signif_earthquakes$eq_mag_unk[is.na(signif_earthquakes$eq_mag_unk)] <- 0
signif_earthquakes <- filter(signif_earthquakes, eq_mag_unk > 0)
```

```{r, echo = TRUE}
head(signif_earthquakes)
```

  
## USGS_df Clean-up

The next step in the cleaning of data was to address the USGS data set from 
Kaggle. Like signif_earthquakes, the columns relevant to this analysis first had
to be extracted. From USGS_df, selected columns were "Date", "Time", "Latitude",
"Longitude", "Depth", and "Magnitude". The selection of these columns allows us 
to complete our data analysis as well as join the columns together.  After 
selecting the relevant columns, the date column was reformatted to international
date format and the "depth" column was renamed to "focal_depth" to better 
indicate what the values represent. The cleaned table for USGS_df appears below:

```{r, echo = FALSE, include = FALSE}
#Import data from USGS on earthquakes larger than 5.5 magnitude from 1965-2016
USGS_earthquakes <- read_csv("database.csv")

#Save USGS data set as a data frame (from csv)
USGS_df <- as.data.frame(USGS_earthquakes)

#Select relevant headings
USGS_df <- select(USGS_df, "Date", "Time", "Latitude", "Longitude", "Depth", 
                  "Magnitude")

#Set column headings to lowercase
USGS_df <- setNames(USGS_df, tolower(names(USGS_df)))

#Format date to YYYY-MM-DD
USGS_df$date <- format(as.Date(USGS_df$date, "%m/%d/%Y"), "%Y-%m-%d")

#Rename depth as "focal_depth"
colnames(USGS_df)[5] <- "focal_depth"
```


```{r}
#Print head(USGS_df)
head(USGS_df)
```

## Joining the data frames   

To best join the data frames together without losing data, a full_join function 
was used. However, for the full_join function to work best, the data from 
signif_earthquakes needed to be slighly altered to match the format found in 
USGS_df. This alteration was done by assigning a zero value to time 
measurements, then by combining hours, minutes, and seconds into one column 
marked "time" in hh:mm:ss format. This zero value was assigned as some 
earthuakes were measured in hh:mm and some in hh:mm:ss.   

Another set of columns that needed to be combined in the signif_earthquakes data
frame were the date columns (month, day, year). These were united into one 
labelled "date" and put into international date format YYYY-MM-DD. These changes
led to the new format of the signif_earthquake data frame:  

```{r, echo = FALSE, include = FALSE}
#Replace NA with zero value in hour, minute, second columns
signif_earthquakes$hour[is.na(signif_earthquakes$hour)] <- 0
signif_earthquakes$minute[is.na(signif_earthquakes$minute)] <- 0
signif_earthquakes$second[is.na(signif_earthquakes$second)] <- 0

#Create date column by joining Year, Month, Day 
signif_earthquakes <- unite(signif_earthquakes, date, c("year", "month", "day"), 
                            sep = "-")

#Create Time Column by joining Hour, Minute, Second
signif_earthquakes <- unite(signif_earthquakes, time, c("hour", "minute", 
                                                        "second"), sep = ":")
signif_earthquakes$time <- hms::as.hms(signif_earthquakes$time)

#Rename eq_mag_unk column as magnitude
colnames(signif_earthquakes)[4] <- "magnitude"

head(signif_earthquakes)
```

Once the two tables were in the correct formats, full_join was used to 
consolidate the data into one data frame: 

```{r}
#Join tables together via date, time, latitude, and longitude
earthquakes <- full_join(signif_earthquakes, USGS_df, by = c("date", "time", 
                                                             "latitude", 
                                                             "longitude", 
                                                             "magnitude", 
                                                             "focal_depth"))

summary(earthquakes)
```

However, one more data set was needed to complete the analysis. Stored as a 
JSON file, this data set contained the plate bounddaries of each plate by 
connecting a series of coordinates. This file was uploaded and read using the 
geojsonio package. Once loaded, the earthquakes data set was overlaid onto the 
plate boundaries data set. 

```{r}

# Import data for tectonic plate boundaries
plate_data <- "PB2002_plates.json"
plates <- geojson_read(plate_data, what = "sp")  

plot(plates)

# convert list of earthquake points into a SpatialPointsDataFrame
coordinates(earthquakes) <- ~ longitude + latitude

# convert earthquakes to use the same coordinate system as plates (for overlay)
proj4string(earthquakes) <- proj4string(plates)

# Create overlay
earthquakes_plates <- over(earthquakes, plates)
```

The final step in tidying the data was to combine the columns from the two 
resulting tables and save the new result as a data frame. This gives us the 
following result: 

```{r}
# Attach the resulting columns that we got from over to the rows of eqs
earthquakes$LAYER <- earthquakes_plates$LAYER
earthquakes$Code <- earthquakes_plates$Code
earthquakes$PlateName <- earthquakes_plates$PlateName

earthquakes <- as.data.frame(earthquakes)


head(earthquakes)
```

##Initial Exploration of Data

After combining the data sets, an initial exploration was completed to identify 
any possible trends. Using the earthquakes, data set, a bar graph was created
for year vs # of major earthquakes from 1965 through 2016. The bar graph yielded
the following results, with a trend of an increasing number of earthquakes 
worldwide. A peak number of earthquakes appears in 2011, which had nearly 100 
more significant earthquakes than any other year in recorded seismic history. 

```{r, echo = FALSE, include = FALSE}

#Bar graph of year vs # of major earthquakes (1965-2016)
ggplot(earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Number of Major Earthquakes per Year")
```

To get more detail on the magnitude of earthquakes that occurred by year, a 
boxplot was created for year against magnitude. Major earthquake outliers above 
a 9.0 magnitude appeared in both 2004 and 2011. However, as the data reveals, 
although there were more earthquakes in 2011, the majority of the earthquakes 
were within the 5.5 to 6.5 magnitude range. 

```{r, echo = FALSE, include= FALSE}
#Boxplot of Year vs. Magnitude
ggplot(earthquakes, aes(x = factor(year), y = magnitude)) +
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Year vs. Magnitude")
```



A boxplot of Plate name vs. magnitude was then created for all earthquakes in 
the data set. From this plot, it is easy to tell that the majority of 
significant earthquakes occuring on all plates register between a 5.5 and a 6.5 
on the Richter scale. This tells us that the outlier events are above a 6.5. The
plates that have extreme outliers (India, Burma, North America, Okhotsk) are 
plates that sit on top of convergent subduction zones, where pressure would 
built until one plate slips under the other, creating a large seismic event. 

```{r, echo = FALSE, include= FALSE}

#Boxplot of Plate name vs magnitude
ggplot(earthquakes, aes(x = PlateName, y = magnitude)) + 
  geom_boxplot() + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Outliers in magnitude on tectonic plates")
```


#Using a map to plot data
The next step taken in the data exploration was to plot the earthquake 
occurences on a world map. Once the map of the tectonic plates was established, 
the earthquakes were then charted by year to see if there was a recognizable 
pattern. 

```{r, echo= FALSE, include= FALSE}
#Map Earthquake locations by magnitude (colored per tectonic plate location)
world <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map() 

plate_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName), data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")

plot(plate_map)

#Map earthquake locations by year and magnitude (color = year, size = magnitude)
year_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")

plot(year_map)
```
As the year plate map revealed, there were many significant earthquakes within 
the last ten years of the data set. To get a better look at the data, the map 
was restricted to just the data from 2006 through 2016. When that data was 
charted, the results were mixed and not quite clear. It became obvious that the 
most recent major earthquakes were along subduction zones such as the western 
edge of the South American continent and along the Aleutian islands of Alaska. 
Places where multiple plates met along the ring of fire (the western, northern, 
and eastern edges of the Pacific plate) experienced strong annual seismic 
events. 

Interestingly, the decade map also picked up increased seismic activity that was
recorded in the middle of the tectonic plates. Some of this, such as the 
Hawaiian Islands, can be caused by 'hot spots' or thin, weak areas in the 
Earth's crust that allow magma to push through, forming a volcano. However, 
other seismic events, such as the ones recorded in arkansas, virginia, and the 
gulf of mexico, may be caused by human activity. As a result, the focus is on 
major earthquakes occurring at plate boundaries. 

```{r}
#Earthquakes from 2006-2016
earthquakes2006_2016 <- filter(earthquakes, year > 2005)

decade_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes2006_2016, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")

plot(decade_map)
```

##Narrowing the data

The next step in the data exploration was to narrow the data. This was done by
restricting the data to just one or two plates and repeating the bar graph and 
the box plots. An animation was also added to better visualize the data. 
The plates that were tested individually were the Pacific plate, the North 
American Plate, the South American Plate, the Eurasian Plate, and the North 
American and Pacific Plates combined. Of these, the only restricted data to show
a pattern was the North American and the Pacific plates. These two plates were 
combined because as the Pacific plate subducts under the North American plate, 
the seismic events are recorded on the North American plate. 

```{r}
#Filter for North American and Pacific plate earthquakes
nap_earthquakes <- filter(earthquakes, PlateName == c("North America", 
                                                      "Pacific"))

#Bar graph of year vs # of major earthquakes (1965-2016)
ggplot(nap_earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Number of Major Earthquakes/Year on N. American & Pacific plates")

#Boxplot of year vs magnitude on North American and Pacific plates
ggplot(nap_earthquakes, aes(x = year, y = magnitude)) + 
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5),
        text=element_text(size=9)) +
  ggtitle("Year vs Magnitude of earthquakes on N. american and Pacific plates")

#Unite month, day, year columns
nap_earthquakes <- unite(nap_earthquakes, quake_date, c("year", "month", "day"),
                         sep = "-")

#Animate progression of earthquakes for North American and Pacific Plates
ghost_points_ini <- tibble(
  quake_date = as.Date('1965-01-01'),
  magnitude = 0, longitude = 0, latitude = 0)

ghost_points_fin <- tibble(
  quake_date = seq(as.Date('2017-01-01'),
                   as.Date('2017-01-02'),
                   by = 'days'),
  magnitude = 0, longitude = 0, latitude = 0)

nap_animated_map <- world +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName, frame = as.Date(quake_date), 
                 cumulative = FALSE), data = head(nap_earthquakes, n = 100L), 
             alpha = .5) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_ini, alpha = 0) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_fin, alpha = 0) +
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")

gganimate::gg_animate(nap_animated_map)
```

After looking at the data and narrowing down interactions to the Pacific and 
North American tectonic plates, the next step was to look for interactions 
between variables. This was done through the following code for both the 
earthquakes data set as well as the narrowed North American/Pacific (NAP) data 
set. 


##Checking for skew of variables

```{r}
hist(earthquakes$focal_depth)
hist(log1p(earthquakes$focal_depth))


ggplot(earthquakes1, aes(x=magnitude)) + geom_density()

ggplot(earthquakes1, aes(x=magnitude)) + geom_density() + 
  scale_x_continuous(trans="log1p")

ggplot(earthquakes1, aes(x=scale(magnitude, center=TRUE, scale=TRUE))) + 
  geom_density()

mean(earthquakes1$magnitude)
median(earthquakes1$magnitude)
```

The next step in the analysis was to check for a skew in the variables; is the 
data skewed to the left or the right? In this case, it was discovered tha the 
focal depth was skewed to the right. The magnitude, although already measured on 
a log scale, was revealed to be minimally skewed to the right. To correct this, 
the focal depth measurements were adjusted by using log1p(focal_depth) in place 
of the direct value of the focal depth. 


##Interactions between variables

After checking for skews, interactions between variables were analyzed. Due to 
the above graphic exploration, the interactions were analyzed across the 
entire data set, earthquakes, and across the restricted North American & Pacific
Plate earthquakes (nap_earthquakes). 

```{r}
##Interactions between variables


pairs(latitude ~ PlateName + log1p(focal_depth), data = earthquakes)


pairs(longitude ~ PlateName + log1p(focal_depth), data = earthquakes)


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = earthquakes)



#Interactions between variables on North_American and Pacific plates


pairs(latitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes)


pairs(longitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes)


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = nap_earthquakes)



```


Given the above interactions, regardless of the data set, the best interaction 
seems to be between longitude and focal depth of the earthquake. Based on the
data, these two variables have the most significant impact on the magnitude of a
major earthquake. This does make sense as many subduction zones run along 
longitudinal lines and an earthquake close to the surface would have a stronger
magnitude than one that is deep below the surface and has to travel through many 
layers of earth prior to reaching the seismographs. 

##Creating predictive models

After analyzing the above interactions, several predictive models were created 
to attempt to better predict the magnitude of major earthquakes. The models that
were used included linear regression, logistic regression, binomial 
regressions, and the Poisson model. 

#Linear Regression
As was discovered earlier, there are multiple variables that can impact an 
earthquake's magnitude. Due to this, multiple linear regression was used to 
attempt to find if there is a linear relationship between the variables. 

```{r}

#Unite month, day, year columns
earthquakes1 <- unite(earthquakes, quake_date, c("year", "month", "day"), 
                      sep = "-")



# Linear Regression Models ------------------------------------------------



LRModel1 <-lm(magnitude ~ latitude + longitude, data = earthquakes1)
summary(LRModel1)

LRModel2 <-lm(magnitude ~ latitude * longitude, data = nap_earthquakes)
summary(LRModel2)

LRModel3 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = earthquakes1)
summary(LRModel3)

LRModel4 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) + PlateName, 
              data = earthquakes1)
summary(LRModel4)

LRModel5 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel5)

LRModel6 <-lm(magnitude ~ latitude * longitude * PlateName, data = earthquakes1)
summary(LRModel6)

LRModel7 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = nap_earthquakes)
summary(LRModel7)

LRModel8 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel8)

LRModel9 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = nap_earthquakes)
summary(LRModel9)
```

Going through the results of the linear regressions, the first thing that is 
noticed is that the data does not follow a normal distribution. The next thing 
that draws attention from this model is the coeffictients vary substantially in 
value, particularly in the models that include individual tectonic plates 
(PlateName). Next, it is clear that for many of the models, the variables 
produce a t-value that is close to zero, demonstrating that the linear 
regression model does not show a linear relationship between the variables. 
Lastly, when looking at the R-squared values, it is clear that as all models are 
close to zero, this method of machine learning is not the best option for this 
data set. 


#Logistic Regression

```{r}
#Create binomial column for major earthquake classification

earthquakes1$MajorEarthquakes = ifelse(earthquakes1$magnitude >= 7.0, 1, 0)
head(earthquakes1)

table(earthquakes1$MajorEarthquakes)

#Null model for earthquakes1

787/18846

#null model is 0.04175952


# Logistic Regression Models ----------------------------------------------


QuakeLog1 <- glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth) ,
                data = earthquakes1, family = binomial)
summary(QuakeLog1)
car::Anova(QuakeLog1)


QuakeLog2 <- glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                data = earthquakes1, family = binomial)
summary(QuakeLog2)
car::Anova(QuakeLog2)



QuakeLog3 <- glm(MajorEarthquakes ~ log1p(focal_depth), data = earthquakes1, 
                family = binomial)
summary(QuakeLog3)
car::Anova(QuakeLog3)



QuakeLog4 <- glm(MajorEarthquakes ~ latitude * longitude, data = earthquakes1, 
                family = binomial)
summary(QuakeLog4)
car::Anova(QuakeLog4)


```
 
Reviewing the models for logistic regression for all earthquakes, it becomes 
clear that a logistic regression does not accurately predict magnitude for 
earthquakes. However, from comparing the outcomes of each Anova test, we find 
longitude to be highly significant in determining whether an earthquakes will 
have a magnitude of 7.0 or above. The tests also reveal focal depth and plate 
name to be fairly significant predictors. These results again make sense as the
plate location of a quake would determine the magnitude of the tremor; for
example,a quake that occurs on the Pacific plate is more likely to be a major 
quake than a tremor on the eurasian plate as the Pacific plate is prone to 
sudden, sharp movements found only in subduction zones.

This process was then repeated by restricting the plates to the NAP plate data.

```{r}
#Create binomial column for major earthquake classification for NAP data

nap_earthquakes$MajorEarthquakes = 
  ifelse(nap_earthquakes$magnitude >= 7.0, 1, 0)
head(nap_earthquakes)

table(nap_earthquakes$MajorEarthquakes)

#Null model for nap_earthquakes

84/1479

#null model is 0.05679513

NapQuakeLog1 <- glm(MajorEarthquakes ~ latitude * longitude, 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog1)
car::Anova(NapQuakeLog1)

NapQuakeLog2 <-glm(MajorEarthquakes ~ latitude * longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog2)
car::Anova(NapQuakeLog2)

NapQuakeLog3 <-glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog3)
car::Anova(NapQuakeLog3)

NapQuakeLog4 <-glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog4)
car::Anova(NapQuakeLog4)

NapQuakeLog5 <-glm(MajorEarthquakes ~ log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog5)
car::Anova(NapQuakeLog5)



#Predicting using the best model(NapQuakeLog4)

predictTrain = predict(NapQuakeLog4, type = "response")
summary(predictTrain)

#Are we predicting higher probabilities for Major Earthquakes (>7.0 magnitude?)

tapply(predictTrain, nap_earthquakes$MajorEarthquakes, mean)
```

Looking at the results of the logistic regression for the North American and 
Pacific plates, it is clear that a logistic regression model better fits this
restricted data set rather than the entire earthquakes set. The deviances are 
1/10 of the values found in the larger set: deviances hover around 600 as do AIC
values, indicating that this is a better fit than when the model is applied to 
all earthquake data. 

Interestingly, when Anova is run across each of the NAP models, we find that
focal depth of the earthquake becomes the most significant predictor of when an 
earthquake's magnitude will exceed 7.0 on the Richter scale. This may be due to 
the earth settling after a large subduction event, leading to a shallower focal
point. 



##Binomial distributions

```{r}

#binomial distribution for earthquakes1
eq_agg_df <- earthquakes1 %>% group_by(PlateName) %>%
  summarize(TotalMajor = sum(MajorEarthquakes), TotalEarthquakes = n())

bd_model <- glm(cbind(TotalMajor, TotalEarthquakes - TotalMajor) ~ PlateName,
    family=binomial("logit"), data=eq_agg_df)

summary(bd_model)

#measuring relative & absolute effects of binomial distributions

exp(coef(bd_model))  #relative effects

plogis(-3.773)
plogis(-3.773 + coef(bd_model)) #absolute effects


#binomial distribution for nap_earthquakes
eq_agg_df_nap <- nap_earthquakes %>% group_by(PlateName) %>% 
  summarize(TotalMajor = sum(MajorEarthquakes), TotalEarthquakes = n())

nap_model <- glm(cbind(TotalMajor, TotalEarthquakes - TotalMajor) ~ PlateName, 
    family = binomial("logit"), data = eq_agg_df_nap)

summary(nap_model)

#measuring relative & absolute effects of binomial distributions for nap data

exp(coef(nap_model))  #relative effects

plogis(-2.82138)
plogis(-2.82138 + coef(nap_model)) #absolute effects

```

From the binomial regression of all earthquakes, it is clear to see that this 
model is a better fit than either the linear reression or the logistic 
regression. Plates that have subduction boundaries have coefficients closest to
zero while plates with little movement or smaller, transverse boundaries have 
large coefficients, indicating their lower likelihood of experiencing a major
tremor. Additionally, we find that the relative and absolute effects of each 
plate in this distribution are minimal, with the majority of values very close 
to zero. This data again makes sense as the effect each plate would have on the 
total of major earthquakes would be minimal given the number of tectonic plates. 
 
However, things get more interesting when applying the binomial distribution to 
the North American/Pacific plates. In this model, the residual deviance is 
extremely low, indicating that this model is a good fit for the data (as opposed
to the binomial regression for the full earthquake data, which had a high 
deviance). When the relative and absolute effects of this model were calculated, 
they were shown to be more significant than the model for all earthquakes; the 
relative effects returned a coefficient of 1.02 while the absolute effect
returned a coefficient of 0.06. These values reinforce the concept that a more 
active plate, such as the Pacific plate, would have a higher impact on the total
of major earthquakes than a less active plate, such as the North American plate
(which usually only experiences major tremors as a result of the Pacific plate 
subducting under it, causing the North American plate to be pushed up).


##Poisson Models

```{r}

#Poisson distribution for earthquakes1

p_model <- glm(TotalMajor ~ PlateName, family=poisson("log"), data=eq_agg_df)

summary(p_model)

exp(coef(p_model))


#Poisson distribution for nap_earthquakes

nap_p_model <- glm(TotalMajor ~ PlateName, family = poisson("log"), 
                 data = eq_agg_df_nap)

summary(nap_p_model)

exp(coef(nap_p_model))
```
Since the Poisson model works better for data that does not 
follow a normal distribution, it is clear from the deviance (close to zero) that
this is the best model for the world earthquake data. Looking at the results of 
the Poisson models, we see there is a very slight increase in the probability of
a major earthquake on each plate using this prediction model.

In applying the model to the restricted NAP data set, we again find a very low
deviance, indicating that this would be a good model for this data set as well.
The coefficient for the Pacific plate in this model gives a 10% probability of a 
major earthquake occurring on this plate. 

##Conclusion

After gathering and analyzing two data sets covering global earthquakes with a
magnitude of 5.5 or higher (as measured by the Richter scale), a relationship 
was found to exist between magnitude and longitude as well as magnitude and 
focal depth. Given what is known about earthquakes, this information is not 
surprising as subduction zones often occur along the longitudinal lines that
create a border along the Pacifc tectonic plate, a part of the Ring of Fire. 

Out of the four types of models created, the best model was the Poisson model. 
The Poisson model gave the probability of occurance of a tremor with a magnitude 
of 7.0 or greater, depending on the tectonic plate location. However, even with 
this probability, there is significant room to improve the possibility of 
earthquake prediction, as the Poisson model does not indicate where the quake 
may occur. 

For future analysis, I would propose a few additions. The first modification to
this analysis would be to analyze the data using a time series model as this 
may show a relationship between the time different earthquakes occurred on a
given plate, as well as their locations. This type of analysis would likely work 
best on a plate that has similar to characteristics the Pacific plate: large 
numbers of significant quakes (magnitude 5.5 or larger), constantly moving, a 
variety of types of fault zones. Another addition would include adding a data 
set that classifies the type of fault zone and to run an analysis on how fault 
zone impacts magnitude. Current geologic knowledge indicates that subduction 
zones cause the largest quakes; however, the amount of time that earthquakes 
have been able to be accurately measured is small when compared to the geologic
timeline and it is possible that a strike-slip fault may also trigger 
less-frequent major quakes. The last addition would be a mixed effects model
where the data would be analyzed to attempt to establish any possible patterns
that may exist between subgroups of data (such as plate location). 

