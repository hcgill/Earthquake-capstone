---
title: "Earthquake_Capstone_Report"
author: "H. Ewton"
date: "9/28/2018"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Table of Contents
Problem
Methods
- Data Sources
- Signif_earthquakes clean-up
- USGS_df clean-up
- Joining the data frames
- Adding plate information
- Exploratory Data Analysis
- Geographic plots (Maps)
- Creation of NAP data set
- Checking for skew of variables
- Interactions between variables
Predictive Models
- Linear Regression
- Logistic Regression
- Binomial Distribution
- Poisson Model
Conclusion


#The Problem

One of the biggest problems that exists in geology involves the prediction of 
significant earthquakes. Earthquakes can be measured in several ways, but a 
significant earthquake is a tremor that measures 5.5 or higher on the Richter 
scale. Predicting such earthquakes is important as higher magnitude earthquakes
often cause significant damage to infrastructure and loss of life. Ideally, a 
solution could be found using data to predict the probability of occurance of a 
significant earthquake for a given location. 

To solve this problem, a significant amount of data would be needed to enable
the creation of a probability prediction. The data would not only need to have
all earthquakes registering above a 5.5 magnitude, but would also need the date
the earthquake occurred, the geographical location (latitude/longitude), the 
focal depth of the quake (the origin of the tremor), and the plate that the 
tremor occurred on/along. This data would then be analyzed first for trends, 
then used to predict the probability of an earthquake occurring on each plate.

#Methods


###Data Sources
Three data sets were used in this analysis. The first data set, 
signif_earthquakes, was obtained from NOAA's Significant Earthquakes Database 
(https://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1) and lists every 
recorded earthquake in history back to 2150 BC. The other data set used, 
USGS_df, was obtained from Kaggle 
(https://www.kaggle.com/usgs/earthquake-database#database.csv) and lists all 
recorded major earthquakes in the USGS data base from 1965 to 2016. 
Additionally, a map of tectonic plate points was used to assign the latitude and
longitude points to a tectonic plate 
(https://github.com/fraxen/tectonicplates/tree/master/GeoJSON).


###Signif_earthquakes clean-up
This file, signif_earthquakes, presented several challenges. After removing 
observations with missing magnitude values and data with estimated magnitude 
(pre-1935), the data was filtered to only include relevant columns: date, time,
magnitude, and location information. 


```{r, echo = FALSE, include = FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(sp)
library(geojsonio)
library(maps)
library(ggthemes)
library(gganimate)
library(caTools)
library(multcomp)

#Import data from NOAA on significant earthquakes in recorded geologic history
signif_earthquakes <- read_delim("signif (3).txt", 
                                 "\t", escape_double = FALSE, trim_ws = TRUE)

#Save as data frame
signif_earthquakes <- as.data.frame(signif_earthquakes, 
                                    stringsAsFactors = FALSE)

#Select relevant headings
signif_earthquakes <- dplyr::select(signif_earthquakes, "YEAR", "MONTH", "DAY", 
                                    "HOUR","MINUTE", "SECOND", "FOCAL_DEPTH",
                                    "EQ_MAG_UNK",  "COUNTRY", "LOCATION_NAME", 
                                    "LATITUDE", "LONGITUDE")

#Change headings to lowercase
signif_earthquakes <- setNames(signif_earthquakes, 
                               tolower(names(signif_earthquakes)))

#Select data from 1965 to present
signif_earthquakes <- filter(signif_earthquakes, year > 1964)

#Remove any magnitude values that are NA 
#(indicates a recorded earthquake with no measurement)
signif_earthquakes$eq_mag_unk[is.na(signif_earthquakes$eq_mag_unk)] <- 0
signif_earthquakes <- filter(signif_earthquakes, eq_mag_unk > 0)
```

```{r, echo = FALSE, include = FALSE}
head(signif_earthquakes)
```

  
### USGS_df Clean-up

The next step in the cleaning of data was to address the USGS data set from 
Kaggle. Like signif_earthquakes, the columns relevant to this analysis first had
to be extracted. From USGS_df, selected columns were "Date", "Time", "Latitude",
"Longitude", "Depth", and "Magnitude". The selection of these columns allows us 
to complete our data analysis as well as join the columns together.  After 
selecting the relevant columns, the date column was reformatted to international
date format and the "depth" column was renamed to "focal_depth" to better 
indicate what the values represent. 

```{r, echo = FALSE, include = FALSE}
#Import data from USGS on earthquakes larger than 5.5 magnitude from 1965-2016
USGS_earthquakes <- read_csv("database.csv")

#Save USGS data set as a data frame (from csv)
USGS_df <- as.data.frame(USGS_earthquakes)

#Select relevant headings
USGS_df <- dplyr::select(USGS_df, "Date", "Time", "Latitude", "Longitude", 
                         "Depth", "Magnitude")

#Set column headings to lowercase
USGS_df <- setNames(USGS_df, tolower(names(USGS_df)))

#Format date to YYYY-MM-DD
USGS_df$date <- format(as.Date(USGS_df$date, "%m/%d/%Y"), "%Y-%m-%d")

#Rename depth as "focal_depth"
colnames(USGS_df)[5] <- "focal_depth"
```


```{r, echo = FALSE, include = FALSE}
#Print head(USGS_df)
head(USGS_df)
```

### Joining the data frames   


To best join the data frames together without losing data, a full_join function 
was used. In this process, the tables were joined by date, time, latitude, 
longitude, magnitude, and focal depth of the earthquakes. 

```{r, echo = FALSE, include = FALSE}
#Replace NA with zero value in hour, minute, second columns
signif_earthquakes$hour[is.na(signif_earthquakes$hour)] <- 0
signif_earthquakes$minute[is.na(signif_earthquakes$minute)] <- 0
signif_earthquakes$second[is.na(signif_earthquakes$second)] <- 0

#Create date column by joining Year, Month, Day 
signif_earthquakes <- unite(signif_earthquakes, date, c("year", "month", "day"), 
                            sep = "-")

#Create Time Column by joining Hour, Minute, Second
signif_earthquakes <- unite(signif_earthquakes, time, c("hour", "minute", 
                                                        "second"), sep = ":")
signif_earthquakes$time <- hms::as.hms(signif_earthquakes$time)

#Rename eq_mag_unk column as magnitude
colnames(signif_earthquakes)[4] <- "magnitude"

head(signif_earthquakes)

#Join tables together via date, time, latitude, and longitude
earthquakes <- full_join(signif_earthquakes, USGS_df, by = c("date", "time", 
                                                             "latitude", 
                                                             "longitude", 
                                                             "magnitude", 
                                                             "focal_depth"))

summary(earthquakes)
```

###Adding plate information

Once the data set was created, one more bit of information was needed for 
analysis- the tectonic plate data. Stored as a JSON file, this data set 
contained the plate boundaries of each plate by connecting a series of 
coordinates. Once loaded, the earthquakes data set was overlaid onto the 
plate boundaries data set. 

```{r, echo = FALSE, include = FALSE}

# Import data for tectonic plate boundaries
plate_data <- "PB2002_plates.json"
plates <- geojson_read(plate_data, what = "sp")  

plot(plates)

# convert list of earthquake points into a SpatialPointsDataFrame
coordinates(earthquakes) <- ~ longitude + latitude

# convert earthquakes to use the same coordinate system as plates (for overlay)
proj4string(earthquakes) <- proj4string(plates)

# Create overlay
earthquakes_plates <- over(earthquakes, plates)

# Attach the resulting columns that we got from over to the rows of eqs
earthquakes$LAYER <- earthquakes_plates$LAYER
earthquakes$Code <- earthquakes_plates$Code
earthquakes$PlateName <- earthquakes_plates$PlateName

earthquakes <- as.data.frame(earthquakes)

#separate date column into year, month, and day
earthquakes <- separate(earthquakes, date, into = c("year", "month", "day"))

#Filter outliers (1 earthquake recorded in 2017, 2018)
earthquakes <- filter(earthquakes, year < 2017)
earthquakes <- filter(earthquakes, magnitude > 5.5)
```

```{r, echo = FALSE}
head(earthquakes)
```


###Exploratory Data Analysis

After combining the data sets, an initial exploration was completed to identify 
any possible trends. Using the earthquakes, data set, a bar graph was created
for year vs # of major earthquakes from 1965 through 2016. The bar graph yielded
the following results, with a trend of an increasing number of earthquakes 
worldwide. A peak number of earthquakes appears in 2011, which had nearly 100 
more significant earthquakes than any other year in recorded seismic history. 

```{r, echo=FALSE}

#Bar graph of year vs # of major earthquakes (1965-2016)
ggplot(earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Number of Major Earthquakes per Year")
```

To get more detail on the magnitude of earthquakes that occurred by year, a 
boxplot was created for year against magnitude. Major earthquake outliers above 
a 9.0 magnitude appeared in both 2004 and 2011. However, as the data reveals, 
although there were more earthquakes in 2011, the majority of the earthquakes 
were within the 5.5 to 6.5 magnitude range. 

```{r, echo = FALSE}
#Boxplot of Year vs. Magnitude
ggplot(earthquakes, aes(x = factor(year), y = magnitude)) +
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Year vs. Magnitude")
```


A boxplot of Plate name vs. magnitude was then created for all earthquakes in 
the data set. From this plot, it is easy to tell that the majority of 
significant earthquakes occuring on all plates register between a 5.5 and a 6.5 
on the Richter scale. This tells us that the outlier events are above a 6.5. The
plates that have extreme outliers (India, Burma, North America, Okhotsk) are 
plates that sit on top of convergent subduction zones, where pressure would 
built until one plate slips under the other, creating a large seismic event. 

```{r, echo = FALSE}

#Boxplot of Plate name vs magnitude
ggplot(earthquakes, aes(x = PlateName, y = magnitude)) + 
  geom_boxplot() + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Outliers in magnitude on tectonic plates")
```


###Geographic Plots
The next step taken in the data exploration was to plot the earthquake 
occurences on a world map. Once the map of the tectonic plates was established, 
the earthquakes were then charted by year to see if there was a recognizable 
pattern. 

```{r, echo= FALSE, include= FALSE}
#Map Earthquake locations by magnitude (colored per tectonic plate location)
world <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map() 

plate_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName), data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")
```


```{r, echo = FALSE}
plot(plate_map)
```

```{r, echo = FALSE, include = FALSE}
#Map earthquake locations by year and magnitude (color = year, size = magnitude)
year_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")
```


```{r, echo = FALSE}
plot(year_map)
```

As the year plate map revealed, there were many significant earthquakes within 
the last ten years of the data set. To get a better look at the data, the map 
was restricted to just the data from 2006 through 2016. When that data was 
charted, the results were mixed and not quite clear. It became obvious that the 
most recent major earthquakes were along subduction zones such as the western 
edge of the South American continent and along the Aleutian islands of Alaska. 
Places where multiple plates met along the ring of fire (the western, northern, 
and eastern edges of the Pacific plate) experienced strong annual seismic 
events. 

Interestingly, the decade map also picked up increased seismic activity that was
recorded in the middle of the tectonic plates. Some of this, such as the 
Hawaiian Islands, can be caused by 'hot spots' or thin, weak areas in the 
Earth's crust that allow magma to push through, forming a volcano. However, 
other seismic events, such as the ones recorded in Arkansas, Virginia, and the 
Gulf of Mexico, may be caused by human activity. As a result, the focus is on 
major earthquakes occurring at plate boundaries. 

```{r, echo = FALSE, include = FALSE}
#Earthquakes from 2006-2016
earthquakes2006_2016 <- filter(earthquakes, year > 2005)

decade_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes2006_2016, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")
```

```{r, echo = FALSE}
plot(decade_map)
```

###Creation of NAP data set

The next step in the data exploration was to narrow the data. This was done by
restricting the data to just one or two plates and repeating the bar graph and 
the box plots. An animation was also added to better visualize the data. 
The plates that were tested individually were the Pacific plate, the North 
American Plate, the South American Plate, the Eurasian Plate, and the North 
American and Pacific Plates combined. Of these, the only restricted data to show
a pattern was the North American and the Pacific plates. These two plates were 
combined because as the Pacific plate subducts under the North American plate, 
the seismic events are recorded on the North American plate. As a result of this
analysis, the North American/Pacific plate data was isolated and analyzed in 
addition to the full data set, earthquakes.

```{r, echo = FALSE, include = FALSE}
#Filter for North American and Pacific plate earthquakes
nap_earthquakes <- filter(earthquakes, PlateName == c("North America", 
                                                      "Pacific"))
```

```{r, echo = FALSE}
#Bar graph of year vs # of major earthquakes (1965-2016)
ggplot(nap_earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Number of Major Earthquakes/Year on N. American & Pacific plates")
```

```{r, echo = FALSE}
#Boxplot of year vs magnitude on North American and Pacific plates
ggplot(nap_earthquakes, aes(x = year, y = magnitude)) + 
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5),
        text=element_text(size=9)) +
  ggtitle("Year vs Magnitude of earthquakes on N. american and Pacific plates")
```

```{r, echo=FALSE, include=FALSE}
#Unite month, day, year columns
nap_earthquakes <- unite(nap_earthquakes, quake_date, c("year", "month", "day"),
                         sep = "-")

#Animate progression of earthquakes for North American and Pacific Plates
ghost_points_ini <- tibble(
  quake_date = as.Date('1965-01-01'),
  magnitude = 0, longitude = 0, latitude = 0)

ghost_points_fin <- tibble(
  quake_date = seq(as.Date('2017-01-01'),
                   as.Date('2017-01-02'),
                   by = 'days'),
  magnitude = 0, longitude = 0, latitude = 0)

nap_animated_map <- world +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName, frame = as.Date(quake_date), 
                 cumulative = FALSE), data = head(nap_earthquakes, n = 100L), 
             alpha = .5) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_ini, alpha = 0) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_fin, alpha = 0) +
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")

#gganimate::gg_animate(nap_animated_map)
```

###Checking for skew of variables

After looking at the data and narrowing down interactions to the Pacific and 
North American tectonic plates, the next step was to look for interactions 
between variables. During this analysis, it was found that the data for focal
depth is slightly skewed to the right, so the log1p() function was used to 
correct this during data analysis.


```{r, echo = FALSE, include = FALSE}
#Unite month, day, year columns
earthquakes1 <- unite(earthquakes, quake_date, c("year", "month", "day"), 
                      sep = "-")

hist(earthquakes$focal_depth)
hist(log1p(earthquakes$focal_depth))


ggplot(earthquakes1, aes(x=magnitude)) + geom_density()

ggplot(earthquakes1, aes(x=magnitude)) + geom_density() + 
  scale_x_continuous(trans="log1p")

ggplot(earthquakes1, aes(x=scale(magnitude, center=TRUE, scale=TRUE))) + 
  geom_density()

mean(earthquakes1$magnitude)
median(earthquakes1$magnitude)
```


###Interactions between variables

After checking for skew, interactions between variables were analyzed across the 
entire data set, earthquakes, and across the restricted North American/Pacific
Plate earthquakes (nap_earthquakes). From the pairs analysis, there appears to
be an interaction between longitude and focal depth.

```{r, echo = FALSE}
##Interactions between variables


pairs(latitude ~ PlateName + log1p(focal_depth), data = earthquakes)


pairs(longitude ~ PlateName + log1p(focal_depth), data = earthquakes)


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = earthquakes)



#Interactions between variables on North_American and Pacific plates


pairs(latitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes)


pairs(longitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes)


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = nap_earthquakes)



```



#Creating predictive models

After analyzing the above interactions, several predictive models were created 
to attempt to better predict the magnitude of major earthquakes. The models that
were used included linear regression, logistic regression, binomial 
regressions, and the Poisson model. 

###Linear Regression

As was discovered earlier, there are multiple variables that can impact an 
earthquake's magnitude. Due to this, multiple linear regression was used to 
attempt to find if there is a linear relationship between the variables. 
However, after completing the linear regressions, it was determined this model
is not successful in predicting the probability of earthquakes as it does not
follow a normal distribution, the coefficients vary substantially in value, and 
the variables produce a t-value that is close to zero, indicating that there is 
not a linear relationship between variables. 


```{r, echo = FALSE, include = FALSE}

#Unite month, day, year columns
earthquakes1 <- unite(earthquakes, quake_date, c("year", "month", "day"), 
                      sep = "-")



# Linear Regression Models ------------------------------------------------



LRModel1 <-lm(magnitude ~ latitude + longitude, data = earthquakes1)
summary(LRModel1)

LRModel2 <-lm(magnitude ~ latitude * longitude, data = nap_earthquakes)
summary(LRModel2)

LRModel3 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = earthquakes1)
summary(LRModel3)

LRModel4 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) + PlateName, 
              data = earthquakes1)
summary(LRModel4)

LRModel5 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel5)

LRModel6 <-lm(magnitude ~ latitude * longitude * PlateName, data = earthquakes1)
summary(LRModel6)

LRModel7 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = nap_earthquakes)
summary(LRModel7)

LRModel8 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel8)

LRModel9 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = nap_earthquakes)
summary(LRModel9)
```



###Logistic Regression

```{r, echo = FALSE, include = FALSE}
#Create binomial column for major earthquake classification

earthquakes1$MajorEarthquakes = ifelse(earthquakes1$magnitude >= 7.0, 1, 0)
head(earthquakes1)

table(earthquakes1$MajorEarthquakes)

#Null model for earthquakes1

787/18846

#null model is 0.04175952


# Logistic Regression Models ----------------------------------------------


QuakeLog1 <- glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth) +
                   PlateName, data = earthquakes1, family = binomial)
summary(QuakeLog1)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(QuakeLog1))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog2 <- glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                 data = earthquakes1, family = binomial)
summary(QuakeLog2)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(QuakeLog2))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog3 <- glm(MajorEarthquakes ~ log1p(focal_depth), data = earthquakes1, 
                 family = binomial)
summary(QuakeLog3)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(QuakeLog3))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog4 <- glm(MajorEarthquakes ~ latitude * longitude, data = earthquakes1, 
                 family = binomial)
summary(QuakeLog4)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(QuakeLog4))
```
 
Reviewing the models for logistic regression for all earthquakes, it becomes 
clear that a logistic regression does not accurately predict magnitude for 
earthquakes. However, from the Wald Type II Chi-sq test, we find longitude to be 
highly significant in determining whether an earthquakes will 
have a magnitude of 7.0 or above. The tests also reveal focal depth and plate 
name to be fairly significant predictors. These results again make sense as the
plate location of a quake would determine the magnitude of the tremor; for
example,a quake that occurs on the Pacific plate is more likely to be a major 
quake than a tremor on the eurasian plate as the Pacific plate is prone to 
sudden, sharp movements found only in subduction zones.

This process was then repeated by restricting the plates to the NAP plate data.

```{r, echo = FALSE, include = FALSE}
#Create binomial column for major earthquake classification for NAP data

nap_earthquakes$MajorEarthquakes = 
  ifelse(nap_earthquakes$magnitude >= 7.0, 1, 0)
head(nap_earthquakes)

table(nap_earthquakes$MajorEarthquakes)

#Null model for nap_earthquakes

84/1479

#null model is 0.05679513

NapQuakeLog1 <- glm(MajorEarthquakes ~ latitude * longitude, 
                    data = nap_earthquakes, family = binomial)
summary(NapQuakeLog1)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog1))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog2 <-glm(MajorEarthquakes ~ latitude * longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog2)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(NapQuakeLog2))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog3 <-glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog3)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog3))
```


```{r, echo=FALSE, include=FALSE}
NapQuakeLog4 <-glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog4)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(NapQuakeLog4))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog5 <-glm(MajorEarthquakes ~ log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog5)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog5))
```


```{r, echo=FALSE, include=FALSE}
#Predicting using the best model(NapQuakeLog4)

predictTrain = predict(NapQuakeLog4, type = "response")
summary(predictTrain)

#Are we predicting higher probabilities for Major Earthquakes (>7.0 magnitude?)

tapply(predictTrain, nap_earthquakes$MajorEarthquakes, mean)
```

Looking at the results of the logistic regression for the North American and 
Pacific plates, it is clear that a logistic regression model better fits this
restricted data set rather than the entire earthquakes set. The deviances are 
1/10 of the values found in the larger set: deviances hover around 600 as do AIC
values, indicating that this is a better fit than when the model is applied to 
all earthquake data. 

Interestingly, when the Wald type II test is run across each of the NAP models,
focal depth of the earthquake becomes the most significant predictor of when an 
earthquake's magnitude will exceed 7.0 on the Richter scale.



###Binomial distributions

```{r, echo = FALSE, include= FALSE}

#binomial distribution for earthquakes1
eq_agg_df <- earthquakes1 %>% group_by(PlateName) %>%
  summarize(TotalMajor = sum(MajorEarthquakes), TotalEarthquakes = n())

bd_model <- glm(cbind(TotalMajor, TotalEarthquakes - TotalMajor) ~ PlateName,
    family=binomial("logit"), data=eq_agg_df)

summary(bd_model)
```

```{r, echo=FALSE}
#measuring relative & absolute effects of binomial distributions

exp(coef(bd_model))  #relative effects

plogis(-3.773)
plogis(-3.773 + coef(bd_model)) #absolute effects
```

```{r, echo=FALSE, include=FALSE}
#binomial distribution for nap_earthquakes
eq_agg_df_nap <- nap_earthquakes %>% group_by(PlateName) %>% 
  summarize(TotalMajor = sum(MajorEarthquakes), TotalEarthquakes = n())

nap_model <- glm(cbind(TotalMajor, TotalEarthquakes - TotalMajor) ~ PlateName, 
    family = binomial("logit"), data = eq_agg_df_nap)

summary(nap_model)
```

```{r, echo=FALSE}
#measuring relative & absolute effects of binomial distributions for nap data

exp(coef(nap_model))  #relative effects

plogis(-2.82138)
plogis(-2.82138 + coef(nap_model)) #absolute effects
```

From the binomial regression of all earthquakes, it is clear to see that this 
model is successful in predicting probability of a major quake depending on the 
plate. Plates that have subduction boundaries have coefficients closest to
zero while plates with little movement or smaller, transverse boundaries have 
large coefficients, indicating their lower likelihood of experiencing a major
tremor. Additionally, we find that the relative and absolute effects of each 
plate in this distribution are minimal, with the majority of values very close 
to zero. 


###Poisson Models

```{r, echo = FALSE}

#Poisson distribution for earthquakes1

p_model <- glm(TotalMajor ~ PlateName, family=poisson("log"), data=eq_agg_df)

summary(p_model)

exp(coef(p_model))


#Poisson distribution for nap_earthquakes

nap_p_model <- glm(TotalMajor ~ PlateName, family = poisson("log"), 
                 data = eq_agg_df_nap)

summary(nap_p_model)

exp(coef(nap_p_model))
```



#Conclusion

After gathering and analyzing two data sets covering global earthquakes with a
magnitude of 5.5 or higher (as measured by the Richter scale), a relationship 
was found to exist between magnitude and longitude as well as magnitude and 
focal depth. Given what is known about earthquakes, this information is not 
surprising as subduction zones often occur along the longitudinal lines that
create a border along the Pacifc tectonic plate, a part of the Ring of Fire. 

Out of the four types of models created, the most successful models were the 
Binomial disribution and the Poisson model. The binomial distribution was able 
to predict the probability of a major quake occurring on a given plate while the 
Poisson model predicted the number of tremors with a magnitude of 7.0 or 
greater, depending on the tectonic plate location. However, even with 
this probability, there is significant room to improve the possibility of 
earthquake prediction, as the Poisson model does not indicate where the quake 
may occur. 

For future analysis, I would propose a few additions. The first modification to
this analysis would be to analyze the data using a time series model as this 
may show a relationship between the time different earthquakes occurred on a
given plate, as well as their locations. This type of analysis would likely work 
best on a plate that has similar to characteristics the Pacific plate: large 
numbers of significant quakes (magnitude 5.5 or larger), constantly moving, a 
variety of types of fault zones. Another addition would include adding a data 
set that classifies the type of fault zone and to run an analysis on how fault 
zone impacts magnitude. Current geologic knowledge indicates that subduction 
zones cause the largest quakes; however, the amount of time that earthquakes 
have been able to be accurately measured is small when compared to the geologic
timeline and it is possible that a strike-slip fault may also trigger 
less-frequent major quakes. The last addition would be a mixed effects model
where the data would be analyzed to attempt to establish any possible patterns
that may exist between subgroups of data (such as plate location). 

#References
