---
title: "Earthquake_Capstone_Report"
author: "H. Ewton"
date: "9/28/2018"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Table of Contents


Problem

Methods

- Data Sources

- Signif_earthquakes clean-up

- USGS_df clean-up

- Joining the data frames

- Adding plate information

- Exploratory Data Analysis

- Geographic plots (Maps)

- Creation of NAP data set

- Checking for skew of variables

- Interactions between variables

Predictive Models

- Linear Regression

- Logistic Regression

- Binomial Distribution

- Poisson Model

Conclusion



#The Problem

One of the biggest problems that exists in geology involves the prediction of 
significant earthquakes. Earthquakes can be measured in several ways, but a 
significant earthquake is a tremor that measures 5.5 or higher on the Richter 
scale. Predicting such earthquakes is important as higher magnitude earthquakes
often cause significant damage to infrastructure and loss of life. Ideally, a 
solution could be found using data to predict the probability of occurance of a 
significant earthquake for a given location. 

To solve this problem, a significant amount of data would be needed to enable
the creation of a probability prediction. The data would not only need to have
all earthquakes registering above a 5.5 magnitude, but would also need the date
the earthquake occurred, the geographical location (latitude/longitude), the 
focal depth of the quake (the origin of the tremor), and the plate that the 
tremor occurred on/along. This data would then be analyzed first for trends, 
then used to predict the probability of an earthquake occurring on each plate.

#Methods


###Data Sources
Three data sets were used in this analysis. The first data set, 
signif_earthquakes, was obtained from [NOAA's Significant Earthquakes Database] 
(https://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1) and lists every 
recorded earthquake in history back to 2150 BC. The other data set used, 
USGS_df, was obtained from [Kaggle] 
(https://www.kaggle.com/usgs/earthquake-database#database.csv) and lists all 
recorded major earthquakes in the USGS data base from 1965 to 2016. 
Additionally, a map of tectonic plate points was used to assign the latitude and
longitude points to a tectonic plate [map of plates]
(https://github.com/fraxen/tectonicplates/tree/master/GeoJSON).


###Signif_earthquakes clean-up
This file, signif_earthquakes, presented several challenges. After removing 
observations with missing magnitude values and data with estimated magnitude 
(pre-1935), the data was filtered to only include relevant columns: date, time,
magnitude, and location information. 


```{r, echo = FALSE, include = FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(sp)
library(geojsonio)
library(maps)
library(ggthemes)
library(gganimate)
library(caTools)
library(multcomp)

#Import data from NOAA on significant earthquakes in recorded geologic history
signif_earthquakes <- read_delim("signif (3).txt", 
                                 "\t", escape_double = FALSE, trim_ws = TRUE)

#Save as data frame
signif_earthquakes <- as.data.frame(signif_earthquakes, 
                                    stringsAsFactors = FALSE)

#Select relevant headings
signif_earthquakes <- dplyr::select(signif_earthquakes, "YEAR", "MONTH", "DAY", 
                                    "HOUR","MINUTE", "SECOND", "FOCAL_DEPTH",
                                    "EQ_MAG_UNK",  "COUNTRY", "LOCATION_NAME", 
                                    "LATITUDE", "LONGITUDE")

#Change headings to lowercase
signif_earthquakes <- setNames(signif_earthquakes, 
                               tolower(names(signif_earthquakes)))

#Select data from 1965 to present
signif_earthquakes <- filter(signif_earthquakes, year > 1964)

#Remove any magnitude values that are NA 
#(indicates a recorded earthquake with no measurement)
signif_earthquakes$eq_mag_unk[is.na(signif_earthquakes$eq_mag_unk)] <- 0
signif_earthquakes <- filter(signif_earthquakes, eq_mag_unk > 0)
```

```{r, echo = FALSE, include = FALSE}
head(signif_earthquakes)
```

  
### USGS_df Clean-up

The next step in the cleaning of data was to address the USGS data set from 
Kaggle. Like signif_earthquakes, the columns relevant to this analysis first had
to be extracted. From USGS_df, selected columns were "Date", "Time", "Latitude",
"Longitude", "Depth", and "Magnitude". The selection of these columns allows us 
to complete our data analysis as well as join the columns together.  After 
selecting the relevant columns, the date column was reformatted to international
date format and the "depth" column was renamed to "focal_depth" to better 
indicate what the values represent. 

```{r, echo = FALSE, include = FALSE}
#Import data from USGS on earthquakes larger than 5.5 magnitude from 1965-2016
USGS_earthquakes <- read_csv("database.csv")

#Save USGS data set as a data frame (from csv)
USGS_df <- as.data.frame(USGS_earthquakes)

#Select relevant headings
USGS_df <- dplyr::select(USGS_df, "Date", "Time", "Latitude", "Longitude", 
                         "Depth", "Magnitude")

#Set column headings to lowercase
USGS_df <- setNames(USGS_df, tolower(names(USGS_df)))

#Format date to YYYY-MM-DD
USGS_df$date <- format(as.Date(USGS_df$date, "%m/%d/%Y"), "%Y-%m-%d")

#Rename depth as "focal_depth"
colnames(USGS_df)[5] <- "focal_depth"
```


```{r, echo = FALSE, include = FALSE}
#Print head(USGS_df)
head(USGS_df)
```

### Joining the data frames   


To best join the data frames together without losing data, a full_join function 
was used. In this process, the tables were joined by date, time, latitude, 
longitude, magnitude, and focal depth of the earthquakes. 

```{r, echo = FALSE, include = FALSE}
#Replace NA with zero value in hour, minute, second columns
signif_earthquakes$hour[is.na(signif_earthquakes$hour)] <- 0
signif_earthquakes$minute[is.na(signif_earthquakes$minute)] <- 0
signif_earthquakes$second[is.na(signif_earthquakes$second)] <- 0

#Create date column by joining Year, Month, Day 
signif_earthquakes <- unite(signif_earthquakes, date, c("year", "month", "day"), 
                            sep = "-")

#Create Time Column by joining Hour, Minute, Second
signif_earthquakes <- unite(signif_earthquakes, time, c("hour", "minute", 
                                                        "second"), sep = ":")
signif_earthquakes$time <- hms::as.hms(signif_earthquakes$time)

#Rename eq_mag_unk column as magnitude
colnames(signif_earthquakes)[4] <- "magnitude"

head(signif_earthquakes)

#Join tables together via date, time, latitude, and longitude
earthquakes <- full_join(signif_earthquakes, USGS_df, by = c("date", "time", 
                                                             "latitude", 
                                                             "longitude", 
                                                             "magnitude", 
                                                             "focal_depth"))

summary(earthquakes)
```

###Adding plate information

Once the data set was created, one more bit of information was needed for 
analysis- the tectonic plate data. Stored as a JSON file, this data set 
contained the plate boundaries of each plate by connecting a series of 
coordinates. Once loaded, the earthquakes data set was overlaid onto the 
plate boundaries data set. 

```{r, echo = FALSE, include = FALSE}

# Import data for tectonic plate boundaries
plate_data <- "PB2002_plates.json"
plates <- geojson_read(plate_data, what = "sp")  

plot(plates)

# convert list of earthquake points into a SpatialPointsDataFrame
coordinates(earthquakes) <- ~ longitude + latitude

# convert earthquakes to use the same coordinate system as plates (for overlay)
proj4string(earthquakes) <- proj4string(plates)

# Create overlay
earthquakes_plates <- over(earthquakes, plates)

# Attach the resulting columns that we got from over to the rows of eqs
earthquakes$LAYER <- earthquakes_plates$LAYER
earthquakes$Code <- earthquakes_plates$Code
earthquakes$PlateName <- earthquakes_plates$PlateName

earthquakes <- as.data.frame(earthquakes)

#separate date column into year, month, and day
earthquakes <- separate(earthquakes, date, into = c("year", "month", "day"))

#Filter outliers (1 earthquake recorded in 2017, 2018)
earthquakes <- filter(earthquakes, year < 2017)
earthquakes <- filter(earthquakes, magnitude > 5.5)
```

```{r, echo=FALSE}
knitr::kable(head(earthquakes))
```


###Exploratory Data Analysis

After combining the data sets, an initial exploration was completed to identify 
any possible trends. Using the earthquakes, data set, a bar graph was created
for year vs # of major earthquakes from 1965 through 2016. The bar graph yielded
the following results, with a trend of an increasing number of earthquakes 
worldwide. A peak number of earthquakes appears in 2011, which had nearly 100 
more significant earthquakes than any other year in recorded seismic history. 

```{r, echo=FALSE}

#Bar graph of # of major earthquakes vs. year (1965-2016)
ggplot(earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Major Earthquakes vs. Year") +
  labs(x = "Year", y = "Number of Major Earthquakes")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  theme(axis.title = element_text(face = "bold"))
```

To get more detail on the magnitude of earthquakes that occurred by year, a 
boxplot was created for year against magnitude. Major earthquake outliers above 
a 9.0 magnitude appeared in both 2004 and 2011. However, as the data reveals, 
although there were more earthquakes in 2011, the majority of the earthquakes 
were within the 5.5 to 6.5 magnitude range. 

```{r, echo = FALSE}
#Boxplot of Magnitude vs. Year
ggplot(earthquakes, aes(x = factor(year), y = magnitude)) +
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Magnitude vs. Year") +
  labs(x = "Year", y = "Magnitude")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  theme(axis.title = element_text(face = "bold"))
```


A boxplot of Plate name vs. magnitude was then created for all earthquakes in 
the data set. From this plot, it is easy to tell that the majority of 
significant earthquakes occuring on all plates register between a 5.5 and a 6.5 
on the Richter scale. This tells us that the outlier events are above a 6.5. The
plates that have extreme outliers (India, Burma, North America, Okhotsk) are 
plates that sit on top of convergent subduction zones, where pressure would 
built until one plate slips under the other, creating a large seismic event. 

```{r, echo = FALSE}

#Boxplot of magnitude vs. plate name
ggplot(earthquakes, aes(x = PlateName, y = magnitude)) + 
  geom_boxplot() + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) +
  ggtitle("Magnitude vs. Plate Name")+
  labs(x = "Tectonic Plate", y = "Magnitude")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  theme(axis.title = element_text(face = "bold"))
```


###Geographic Plots
The next step taken in the data exploration was to plot the earthquake 
occurences on a world map. Once the map of the tectonic plates was established, 
the earthquakes were then charted by year to see if there was a recognizable 
pattern. 

```{r, echo= FALSE, include= FALSE}
#Map Earthquake locations by magnitude (colored per tectonic plate location)
world <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map() 

plate_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName), data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5))
```


```{r, echo = FALSE}
plot(plate_map)
```

```{r, echo = FALSE, include = FALSE}
#Map earthquake locations by year and magnitude (color = year, size = magnitude)
year_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")
```


```{r, echo = FALSE}
plot(year_map)
```

As the year plate map revealed, there were many significant earthquakes within 
the last ten years of the data set. To get a better look at the data, the map 
was restricted to just the data from 2006 through 2016. When that data was 
charted, the results were mixed and not quite clear. It became obvious that the 
most recent major earthquakes were along subduction zones such as the western 
edge of the South American continent and along the Aleutian islands of Alaska. 
Places where multiple plates met along the ring of fire (the western, northern, 
and eastern edges of the Pacific plate) experienced strong annual seismic 
events. 

Interestingly, the decade map also picked up increased seismic activity that was
recorded in the middle of the tectonic plates. Some of this, such as the 
Hawaiian Islands, can be caused by 'hot spots' or thin, weak areas in the 
Earth's crust that allow magma to push through, forming a volcano. However, 
other seismic events, such as the ones recorded in Arkansas, Virginia, and the 
Gulf of Mexico, may be caused by human activity. As a result, the focus is on 
major earthquakes occurring at plate boundaries. 

```{r, echo = FALSE, include = FALSE}
#Earthquakes from 2006-2016
earthquakes2006_2016 <- filter(earthquakes, year > 2005)

decade_map <- world +
  geom_point(aes(x = longitude, y = latitude, size = magnitude, color = year), 
             data = earthquakes2006_2016, alpha = .5) +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Year') +
  theme(legend.position = "right")
```

```{r, echo = FALSE}
plot(decade_map)
```

###Creation of NAP data set

The next step in the data exploration was to narrow the data. This was done by
restricting the data to just one or two plates and repeating the bar graph and 
the box plots. An animation was also added to better visualize the data. 
The plates that were tested individually were the Pacific plate, the North 
American Plate, the South American Plate, the Eurasian Plate, and the North 
American and Pacific Plates combined. Of these, the only restricted data to show
a pattern was the North American and the Pacific plates. These two plates were 
combined because as the Pacific plate subducts under the North American plate, 
the seismic events are recorded on the North American plate. As a result of this
analysis, the North American/Pacific plate data was isolated and analyzed in 
addition to the full data set, earthquakes.

```{r, echo = FALSE, include = FALSE}
#Filter for North American and Pacific plate earthquakes
nap_earthquakes <- filter(earthquakes, PlateName == c("North America", 
                                                      "Pacific"))
```

```{r, echo = FALSE}
#Bar graph of # of major earthquakes vs. year (1965-2016)
ggplot(nap_earthquakes, aes(factor(year))) + 
  geom_bar(stat = "count", fill = "dark red") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5), 
        text=element_text(size=9)) + 
  ggtitle("Number of Major Earthquakes/Year on N. American & Pacific plates")+
  labs(x = "Year", y = "Number of Major Earthquakes")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  theme(axis.title = element_text(face = "bold"))
```

```{r, echo = FALSE}
#Boxplot of magnitude vs year on North American and Pacific plates
ggplot(nap_earthquakes, aes(x = year, y = magnitude)) + 
  geom_boxplot() +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5),
        text=element_text(size=9)) +
  ggtitle("Magnitude vs Year of earthquakes on N. american and Pacific plates")+
  labs(x = "Year", y = "Magnitude")+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  theme(axis.title = element_text(face = "bold"))
```

```{r, echo=FALSE, include=FALSE}
#Unite month, day, year columns
nap_earthquakes <- unite(nap_earthquakes, quake_date, c("year", "month", "day"),
                         sep = "-")

#Animate progression of earthquakes for North American and Pacific Plates
ghost_points_ini <- tibble(
  quake_date = as.Date('1965-01-01'),
  magnitude = 0, longitude = 0, latitude = 0)

ghost_points_fin <- tibble(
  quake_date = seq(as.Date('2017-01-01'),
                   as.Date('2017-01-02'),
                   by = 'days'),
  magnitude = 0, longitude = 0, latitude = 0)

nap_animated_map <- world +
  geom_point(data = plates, aes(x = long, y = lat), fill = 'black', stroke = 1)+
  geom_point(aes(x = longitude, y = latitude, size = magnitude, 
                 color = PlateName, frame = as.Date(quake_date), 
                 cumulative = FALSE), data = head(nap_earthquakes, n = 100L), 
             alpha = .5) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_ini, alpha = 0) +
  geom_point(aes(x = longitude, y = latitude, size = magnitude,  
                 frame = quake_date,
                 cumulative = FALSE),
             data = ghost_points_fin, alpha = 0) +
  scale_size_continuous(range = c(1, 8), breaks = c(6, 7, 8, 9)) +
  labs(size = 'magnitude', color = 'Plate Name') +
  theme(legend.position = "right")

#gganimate::gg_animate(nap_animated_map)
```

###Checking for skew of variables

After looking at the data and narrowing down interactions to the Pacific and 
North American tectonic plates, the next step was to look for interactions 
between variables. During this analysis, it was found that the data for focal
depth is slightly skewed to the right, so the log1p() function was used to 
correct this during data analysis.


```{r, echo = FALSE, include = FALSE}
#Unite month, day, year columns
earthquakes1 <- unite(earthquakes, quake_date, c("year", "month", "day"), 
                      sep = "-")

hist(earthquakes$focal_depth)
hist(log1p(earthquakes$focal_depth))


ggplot(earthquakes1, aes(x=magnitude)) + geom_density()

ggplot(earthquakes1, aes(x=magnitude)) + geom_density() + 
  scale_x_continuous(trans="log1p")

ggplot(earthquakes1, aes(x=scale(magnitude, center=TRUE, scale=TRUE))) + 
  geom_density()

mean(earthquakes1$magnitude)
median(earthquakes1$magnitude)
```


###Interactions between variables

After checking for skew, interactions between variables were analyzed across the 
entire data set, earthquakes, and across the restricted North American/Pacific
Plate earthquakes (nap_earthquakes). In the pairs analysis, the red circles
represent any earthquake with a magnitude of 7.0 or higher. 

For the full earthquakes data set, the first pairs chart did not reveal a 
discernible relationship between latitude, plate name, and focal depth. However,
from the second and third pairs plots, there does appear to be a relationship 
between plate name and longitude, as well as plate name and focal depth. 
Additionally, there appears to be a relationship between magnitude and 
focal_depth, with the strongest quakes occurring within a narrow depth range. 

In the pairs plot for the North America/Paccific data set, the majority of 
quakes are clustered around specific latitudes and longitudes. These plots 
reveal that the majority of quakes occur along the edge of the plate, with 
few significant quakes towards the center of the plate. This plot also shows
that the majority of earthquakes with a magnitude of 7.0 or above occur within
a fairly narrow focal depth range. 

```{r, echo = FALSE}
##Interactions between variables


pairs(latitude ~ PlateName + log1p(focal_depth), data = earthquakes, 
      col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
      pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))


pairs(longitude ~ PlateName + log1p(focal_depth), data = earthquakes, 
      col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
      pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = earthquakes, 
     col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
     pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))



#Interactions between variables on North_American and Pacific plates


pairs(latitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes, 
      col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
      pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))


pairs(longitude ~ PlateName + log1p(focal_depth), data = nap_earthquakes, 
      col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
      pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))


pairs(magnitude ~ latitude + longitude + PlateName + log1p(focal_depth), 
     data = nap_earthquakes, 
     col=ifelse(earthquakes$magnitude >= 7.0, "red", "black"), 
     pch=ifelse(earthquakes$magnitude >= 7.0, 16, 17))


```



#Creating predictive models

After analyzing the above interactions, several predictive models were created 
to attempt to better predict the magnitude of major earthquakes. The models that
were used included linear regression, logistic regression, binomial 
regressions, and the Poisson model. 

###Linear Regression

As was discovered earlier, there are multiple variables that can impact an 
earthquake's magnitude. Due to this, multiple linear regression was used to 
attempt to find if there is a linear relationship between the variables and if
that relationship could be used to predict the magnitude of a quake. However, 
after completing the linear regressions,it was determined that this model was 
not successful in predicting magnitude of a quake as the likelihood 
and errors given by the models are not normally distributed. 

```{r, echo = FALSE, include = FALSE}

#Unite month, day, year columns
earthquakes1 <- unite(earthquakes, quake_date, c("year", "month", "day"), 
                      sep = "-")



# Linear Regression Models ------------------------------------------------



LRModel1 <-lm(magnitude ~ latitude + longitude, data = earthquakes1)
summary(LRModel1)

LRModel2 <-lm(magnitude ~ latitude * longitude, data = nap_earthquakes)
summary(LRModel2)

LRModel3 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = earthquakes1)
summary(LRModel3)

LRModel4 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) + PlateName, 
              data = earthquakes1)
summary(LRModel4)

LRModel5 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel5)

LRModel6 <-lm(magnitude ~ latitude * longitude * PlateName, data = earthquakes1)
summary(LRModel6)

LRModel7 <-lm(magnitude ~ latitude * longitude + log1p(focal_depth), 
              data = nap_earthquakes)
summary(LRModel7)

LRModel8 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = earthquakes1)
summary(LRModel8)

LRModel9 <-lm(magnitude ~ latitude * longitude * log1p(focal_depth) * PlateName, 
              data = nap_earthquakes)
summary(LRModel9)
```



###Logistic Regression

```{r, echo = FALSE, include = FALSE}
#Create binomial column for major earthquake classification

earthquakes1$MajorEarthquakes = ifelse(earthquakes1$magnitude >= 7.0, 1, 0)
head(earthquakes1)

table(earthquakes1$MajorEarthquakes)

#Null model for earthquakes1

787/18846

#null model is 0.04175952


# Logistic Regression Models ----------------------------------------------


QuakeLog1 <- glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth) +
                   PlateName, data = earthquakes1, family = binomial)
summary(QuakeLog1)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(QuakeLog1))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog2 <- glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                 data = earthquakes1, family = binomial)
summary(QuakeLog2)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(QuakeLog2))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog3 <- glm(MajorEarthquakes ~ log1p(focal_depth), data = earthquakes1, 
                 family = binomial)
summary(QuakeLog3)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(QuakeLog3))
```


```{r, echo=FALSE, include=FALSE}
QuakeLog4 <- glm(MajorEarthquakes ~ latitude * longitude, data = earthquakes1, 
                 family = binomial)
summary(QuakeLog4)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(QuakeLog4))
```
 
Reviewing the models for logistic regression for all earthquakes, it becomes 
clear from the Wald Type II Chi-sq test that longitude is highly significant in 
determining whether an earthquakes will have a magnitude of 7.0 or above. The 
tests also reveal focal depth and plate name to be fairly significant 
predictors.

This process was then repeated by restricting the plates to the NAP plate data.

```{r, echo = FALSE, include = FALSE}
#Create binomial column for major earthquake classification for NAP data

nap_earthquakes$MajorEarthquakes = 
  ifelse(nap_earthquakes$magnitude >= 7.0, 1, 0)
head(nap_earthquakes)

table(nap_earthquakes$MajorEarthquakes)

#Null model for nap_earthquakes

84/1479

#null model is 0.05679513

NapQuakeLog1 <- glm(MajorEarthquakes ~ latitude * longitude, 
                    data = nap_earthquakes, family = binomial)
summary(NapQuakeLog1)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog1))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog2 <-glm(MajorEarthquakes ~ latitude * longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog2)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(NapQuakeLog2))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog3 <-glm(MajorEarthquakes ~ latitude + longitude + log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog3)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog3))
```


```{r, echo=FALSE, include=FALSE}
NapQuakeLog4 <-glm(MajorEarthquakes ~ longitude * log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog4)
```

```{r, echo=FALSE}
knitr::kable(car::Anova(NapQuakeLog4))
```

```{r, echo=FALSE, include=FALSE}
NapQuakeLog5 <-glm(MajorEarthquakes ~ log1p(focal_depth), 
                   data = nap_earthquakes, family = binomial)
summary(NapQuakeLog5)
```

```{r, echo = FALSE}
knitr::kable(car::Anova(NapQuakeLog5))
```


```{r, echo=FALSE, include=FALSE}
#Predicting using the best model(NapQuakeLog4)

predictTrain = predict(NapQuakeLog4, type = "response")
summary(predictTrain)

#Are we predicting higher probabilities for Major Earthquakes (>7.0 magnitude?)

tapply(predictTrain, nap_earthquakes$MajorEarthquakes, mean)
```

Looking at the results of the logistic regression for the North American and 
Pacific plates, the results are similar. The deviances are approximately 1/10 of 
the values found in the larger set: deviances hover around 600 as do AIC
values. This makes sense as the data for the Pacific plate is roughly 1/10 of 
the total data from the earthquakes data set. 

Interestingly, when the Wald type II test is run across each of the NAP models,
focal depth of the earthquake becomes the most significant predictor of when an 
earthquake's magnitude will exceed 7.0 on the Richter scale, as compared to 
longitude in the full data set. 



###Binomial distributions

```{r, echo = FALSE, include= FALSE}

#binomial distribution for earthquakes1
eq_agg_df <- earthquakes1 %>% group_by(PlateName) %>%
  summarize(TotalMajor = sum(MajorEarthquakes), TotalEarthquakes = n())

bd_model <- glm(cbind(TotalMajor, TotalEarthquakes - TotalMajor) ~ PlateName,
    family=binomial("logit"), data=eq_agg_df)

summary(bd_model)
```

```{r, echo=FALSE}
#measuring relative & absolute effects of binomial distributions

knitr::kable(exp(coef(bd_model)))  #relative effects

plogis(-3.773)
knitr::kable(plogis(-3.773 + coef(bd_model))) #absolute effects
```

Given the sample size, binomial regression was also used to predict the 
probability of a significant earthquake. This type of model was selected because
each plate has a different number of total earthquakes and major earthquakes; a
binomial regression preserves thesesample sizes and the variability.  In the 
binomial regression, both relative and absolute effects were tested to give 
probabilities for each plate.

In the relative effect, values represent how adding plate information increases 
the odds of a major earthquake. From the relative effect model, we can see the 
Scotia plate, the South American plate, the North Bismark plate, the Pacific 
plate, the North Andes plate, and the New Hebrides plate all significantly 
increase the odds of a major earthquake. 

In the absolute effect, the probabilities of a plate being affected by a major 
earthquakes were calculated. From this calculation, it is obvious which plates
have the highest probability of experiencing a major tremor: the Amur plate, the
Molucca Sea plate, the Maoke plate, the India plate, the North Andes plate, the
Juan Fernandez plate, the Scotia plate, the Panama Plate, and the Solomon Sea 
plate all have probabilities higher than 6.5%. Of these, the Solomon Sea plate
has the highest probability of experiencing a major earthquake at 8.19%. 

###Poisson Models

```{r, echo = FALSE, include=FALSE}

#Poisson distribution for earthquakes1

p_model <- glm(TotalMajor ~ PlateName, family=poisson("log"), data=eq_agg_df)

summary(p_model)
```

```{r, echo=FALSE}
knitr::kable(exp(coef(p_model)))
```

From the Poisson model, it becomes clear which plates are likely to have the 
highest number of earthquakes: the Pacific plate is projected to have 42 major
earthquakes while the South American plate comes in with a close 38.5 projected
major quakes. The Sunda plate, under southeast Asia and western Indonesia is 
also predicted to experience a large volume of significant tremors, with a 
predicted 31.5 earthquakes. 

#Conclusion

After gathering and analyzing two data sets covering global earthquakes with a
magnitude of 5.5 or higher (as measured by the Richter scale), a relationship 
was found to exist between magnitude and longitude as well as magnitude and 
focal depth. Given what is known about earthquakes, this information is not 
surprising as subduction zones often occur along the longitudinal lines that
create a border along the Pacifc tectonic plate, a part of the Ring of Fire. 

Out of the four types of models created, the most successful models were the 
Binomial disribution and the Poisson model. The binomial distribution was able 
to predict the probability of a major quake occurring on a given plate while the 
Poisson model predicted the number of tremors with a magnitude of 7.0 or 
greater, depending on the tectonic plate location. From the binomial regression, 
the Solomon Sea plate was found to have the highest probability of experiencing 
a major earthquake. The plates around the Solomon sea plate, such as the New
Hebrides plate and the Molucca Sea plate are also found to have high 
probabilities of a major quake. From the Poisson model, it was predecited that 
the Pacific plate would have the highest number of major earthquakes (42). 
However, even with this probability, there is significant room to improve the 
possibility of earthquake prediction, as the Binomial regression and Poisson 
models do not indicate where the quake may occur. 

For future analysis, I would propose a few additions. The first modification to
this analysis would be to analyze the data using a time series model as this 
may show a relationship between the time different earthquakes occurred on a
given plate, as well as their locations. This type of analysis would likely work 
best on a plate that has similar to characteristics the Pacific plate: large 
numbers of significant quakes (magnitude 5.5 or larger), constantly moving, a 
variety of types of fault zones. Another addition would include adding a data 
set that classifies the type of fault zone and to run an analysis on how fault 
zone impacts magnitude. Current geologic knowledge indicates that subduction 
zones cause the largest quakes; however, the amount of time that earthquakes 
have been able to be accurately measured is small when compared to the geologic
timeline and it is possible that a strike-slip fault may also trigger 
less-frequent major quakes. The last addition would be a mixed effects model
to maximize the data by better accounting for variances in plate measurements, 
geological activy, and geological formations not captured directly in the
measurement values. 

#References

Ahlenius, Hugo. “Fraxen/Tectonicplates.” Tectonicplates/GeoJSON, Github, 2 Oct. 2014, github.com/fraxen/tectonicplates/tree/master/GeoJSON.

National Oceanic and Atmospheric Administration. "NCEI/WDS Global Significant
Earthquake Database." 10 June 2017.
https://www.ngdc.noaa.gov/nndc/struts/form?t=101650&s=1&d=1

US Geological Survey. “Significant Earthquakes, 1965-2016.” Significant 
Earthquakes, 1965-2016, 26 Jan. 2017, www.kaggle.com/usgs/earthquake-database.
